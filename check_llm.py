#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ LLM
"""

import asyncio
import aiohttp
import os
import sys

async def check_llm_status():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ LLM"""
    ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
    model_name = os.getenv('OLLAMA_MODEL', 'llama2:7b')
    
    print(f"=== –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ LLM ===")
    print(f"Ollama URL: {ollama_url}")
    print(f"–ú–æ–¥–µ–ª—å: {model_name}")
    print()
    
    try:
        async with aiohttp.ClientSession() as session:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            print("1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama...")
            async with session.get(f"{ollama_url}/api/tags") as response:
                if response.status == 200:
                    print("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–Ω–∞")
                    data = await response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    
                    if model_name in models:
                        print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –Ω–∞–π–¥–µ–Ω–∞")
                        
                        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
                        print("2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
                        test_payload = {
                            "model": model_name,
                            "prompt": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
                            "stream": False,
                            "options": {
                                "temperature": 0.7,
                                "max_tokens": 50
                            }
                        }
                        
                        async with session.post(
                            f"{ollama_url}/api/generate",
                            json=test_payload,
                            timeout=aiohttp.ClientTimeout(total=10)
                        ) as test_response:
                            if test_response.status == 200:
                                test_data = await test_response.json()
                                response_text = test_data.get('response', '').strip()
                                print("‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç!")
                                print(f"   –¢–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç: {response_text}")
                                return True
                            else:
                                print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {test_response.status}")
                                return False
                    else:
                        print(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(models) if models else '–Ω–µ—Ç'}")
                        return False
                else:
                    print(f"‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {response.status}")
                    return False
                    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    success = await check_llm_status()
    
    print()
    if success:
        print("üéâ LLM –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        sys.exit(0)
    else:
        print("üí• LLM –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        print("\n–í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:")
        print("1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞")
        print("2. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: ollama pull llama2:7b")
        print("3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è OLLAMA_URL –∏ OLLAMA_MODEL")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 